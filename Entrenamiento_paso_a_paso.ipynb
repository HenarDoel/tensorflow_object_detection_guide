{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mimages\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtest\u001b[00m\r\n",
      "│   │   ├── \u001b[01;34mJPEGImages\u001b[00m\r\n",
      "│   │   └── \u001b[01;34mVisualization\u001b[00m\r\n",
      "│   ├── \u001b[01;34mtrain\u001b[00m\r\n",
      "│   │   ├── \u001b[01;34mJPEGImages\u001b[00m\r\n",
      "│   │   └── \u001b[01;34mVisualization\u001b[00m\r\n",
      "│   └── \u001b[01;34mval\u001b[00m\r\n",
      "│       ├── \u001b[01;34mJPEGImages\u001b[00m\r\n",
      "│       └── \u001b[01;34mVisualization\u001b[00m\r\n",
      "├── \u001b[01;34mpretrained_models\u001b[00m\r\n",
      "│   ├── \u001b[01;34mfaster_rcnn_inception_v2_coco_2018_01_28\u001b[00m\r\n",
      "│   │   └── \u001b[01;34msaved_model\u001b[00m\r\n",
      "│   └── \u001b[01;34mmask_rcnn_inception_v2_coco_2018_01_28\u001b[00m\r\n",
      "│       └── \u001b[01;34msaved_model\u001b[00m\r\n",
      "├── \u001b[01;34mscripts\u001b[00m\r\n",
      "└── \u001b[01;34mtraining\u001b[00m\r\n",
      "\r\n",
      "17 directories\r\n"
     ]
    }
   ],
   "source": [
    "!tree -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "succesfully created validation split using:  15.0 % of data\n",
      "succesfully created test split using:  10.0 % of data\n",
      "succesfully created train split using:  75.0 % of data\n"
     ]
    }
   ],
   "source": [
    "# vamos a dividir nuestras imágenes y anotaciones en splits de train, test y validación.\n",
    "# los argumentos -vr y -tr indican respectivamente los porcentajes de datos reservados para validación y test\n",
    "!python3 scripts/train_test_split.py -i images -o images -tr 0.1 -vr 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset: images/train/coco\n",
      "Generating dataset from: images/train/images_frame2000.json\n",
      "Generating dataset from: images/train/images_frame8050.json\n",
      "Generating dataset from: images/train/images_frame2750.json\n",
      "Generating dataset from: images/train/images_frame7900.json\n",
      "Generating dataset from: images/train/images_frame5800.json\n",
      "Generating dataset from: images/train/images_frame10250.json\n",
      "Generating dataset from: images/train/images_frame2950.json\n",
      "Generating dataset from: images/train/images_frame3750.json\n",
      "Generating dataset from: images/train/images_frame10150.json\n",
      "Generating dataset from: images/train/images_frame5200.json\n",
      "Generating dataset from: images/train/images_frame1700.json\n",
      "Generating dataset from: images/train/images_frame5150.json\n",
      "Generating dataset from: images/train/images_frame10050.json\n",
      "Generating dataset from: images/train/images_frame7350.json\n",
      "Generating dataset from: images/train/images_frame1350.json\n",
      "Generating dataset from: images/train/images_frame3350.json\n",
      "Generating dataset from: images/train/images_frame8500.json\n",
      "Generating dataset from: images/train/images_frame1750.json\n",
      "Generating dataset from: images/train/images_frame1550.json\n",
      "Generating dataset from: images/train/images_frame9200.json\n",
      "Generating dataset from: images/train/images_frame2300.json\n",
      "Generating dataset from: images/train/images_frame10100.json\n",
      "Generating dataset from: images/train/images_frame9650.json\n",
      "Generating dataset from: images/train/images_frame8800.json\n",
      "Generating dataset from: images/train/images_frame3150.json\n",
      "Generating dataset from: images/train/images_frame5450.json\n",
      "Generating dataset from: images/train/images_frame3250.json\n",
      "Generating dataset from: images/train/images_frame9100.json\n",
      "Generating dataset from: images/train/images_frame3300.json\n",
      "Generating dataset from: images/train/images_frame9750.json\n",
      "Generating dataset from: images/train/images_frame9000.json\n",
      "Generating dataset from: images/train/images_frame9950.json\n",
      "Generating dataset from: images/train/images_frame8450.json\n",
      "Generating dataset from: images/train/images_frame8400.json\n",
      "Generating dataset from: images/train/images_frame2350.json\n",
      "Generating dataset from: images/train/images_frame2500.json\n",
      "Generating dataset from: images/train/images_frame1650.json\n",
      "Generating dataset from: images/train/images_frame9450.json\n",
      "Generating dataset from: images/train/images_frame2100.json\n",
      "Generating dataset from: images/train/images_frame4850.json\n",
      "Generating dataset from: images/train/images_frame9600.json\n",
      "Generating dataset from: images/train/images_frame4550.json\n",
      "Generating dataset from: images/train/images_frame7650.json\n",
      "Generating dataset from: images/train/images_frame7000.json\n",
      "Generating dataset from: images/train/images_frame1450.json\n",
      "Generating dataset from: images/train/images_frame8150.json\n",
      "Generating dataset from: images/train/images_frame5250.json\n",
      "Generating dataset from: images/train/images_frame6950.json\n",
      "Generating dataset from: images/train/images_frame4500.json\n",
      "Generating dataset from: images/train/images_frame1000.json\n",
      "Generating dataset from: images/train/images_frame4400.json\n",
      "Generating dataset from: images/train/images_frame2600.json\n",
      "Generating dataset from: images/train/images_frame3550.json\n",
      "Generating dataset from: images/train/images_frame2650.json\n",
      "Generating dataset from: images/train/images_frame7750.json\n",
      "Generating dataset from: images/train/images_frame8900.json\n",
      "Generating dataset from: images/train/images_frame8250.json\n",
      "Generating dataset from: images/train/images_frame1300.json\n",
      "Generating dataset from: images/train/images_frame7600.json\n",
      "Generating dataset from: images/train/images_frame6900.json\n",
      "Generating dataset from: images/train/images_frame5500.json\n",
      "Generating dataset from: images/train/images_frame7700.json\n",
      "Generating dataset from: images/train/images_frame2900.json\n",
      "Generating dataset from: images/train/images_frame3400.json\n",
      "Generating dataset from: images/train/images_frame5350.json\n",
      "Generating dataset from: images/train/images_frame4950.json\n",
      "Generating dataset from: images/train/images_frame5550.json\n",
      "Generating dataset from: images/train/images_frame10000.json\n",
      "Generating dataset from: images/train/images_frame8850.json\n",
      "Generating dataset from: images/train/images_frame7550.json\n",
      "Generating dataset from: images/train/images_frame8750.json\n",
      "Generating dataset from: images/train/images_frame950.json\n",
      "Generating dataset from: images/train/images_frame7200.json\n",
      "Generating dataset from: images/train/images_frame7400.json\n",
      "Generating dataset from: images/train/images_frame7300.json\n",
      "Generating dataset from: images/train/images_frame1400.json\n",
      "Generating dataset from: images/train/images_frame7100.json\n",
      "Generating dataset from: images/train/images_frame9050.json\n",
      "Generating dataset from: images/train/images_frame8350.json\n",
      "Generating dataset from: images/train/images_frame7950.json\n",
      "Generating dataset from: images/train/images_frame3450.json\n",
      "Generating dataset from: images/train/images_frame4900.json\n",
      "Generating dataset from: images/train/images_frame9500.json\n",
      "Generating dataset from: images/train/images_frame4350.json\n",
      "Generating dataset from: images/train/images_frame2250.json\n",
      "Generating dataset from: images/train/images_frame7500.json\n",
      "Generating dataset from: images/train/images_frame3000.json\n",
      "Generating dataset from: images/train/images_frame4200.json\n",
      "Generating dataset from: images/train/images_frame9150.json\n",
      "Generating dataset from: images/train/images_frame7150.json\n",
      "Generating dataset from: images/train/images_frame2450.json\n",
      "Generating dataset from: images/train/images_frame2550.json\n",
      "Generating dataset from: images/train/images_frame2150.json\n",
      "Generating dataset from: images/train/images_frame1500.json\n",
      "Generating dataset from: images/train/images_frame9550.json\n",
      "Generating dataset from: images/train/images_frame1100.json\n",
      "Generating dataset from: images/train/images_frame8950.json\n",
      "Generating dataset from: images/train/images_frame4100.json\n",
      "Generating dataset from: images/train/images_frame8200.json\n",
      "Generating dataset from: images/train/images_frame9300.json\n",
      "Generating dataset from: images/train/images_frame4750.json\n",
      "Generating dataset from: images/train/images_frame8100.json\n",
      "Generating dataset from: images/train/images_frame7800.json\n",
      "Generating dataset from: images/train/images_frame2700.json\n",
      "Generating dataset from: images/train/images_frame9900.json\n",
      "Generating dataset from: images/train/images_frame3850.json\n",
      "Generating dataset from: images/train/images_frame3950.json\n",
      "Generating dataset from: images/train/images_frame7250.json\n",
      "Generating dataset from: images/train/images_frame8300.json\n",
      "Generating dataset from: images/train/images_frame10300.json\n",
      "Generating dataset from: images/train/images_frame3500.json\n",
      "Generating dataset from: images/train/images_frame8650.json\n",
      "Generating dataset from: images/train/images_frame4150.json\n",
      "Generating dataset from: images/train/images_frame4650.json\n",
      "Generating dataset from: images/train/images_frame3650.json\n",
      "Generating dataset from: images/train/images_frame1150.json\n",
      "Generating dataset from: images/train/images_frame3900.json\n",
      "Creating dataset: images/test/coco\n",
      "Generating dataset from: images/test/images_frame3100.json\n",
      "Generating dataset from: images/test/images_frame2050.json\n",
      "Generating dataset from: images/test/images_frame7450.json\n",
      "Generating dataset from: images/test/images_frame900.json\n",
      "Generating dataset from: images/test/images_frame5700.json\n",
      "Generating dataset from: images/test/images_frame9700.json\n",
      "Generating dataset from: images/test/images_frame3700.json\n",
      "Generating dataset from: images/test/images_frame1950.json\n",
      "Generating dataset from: images/test/images_frame3800.json\n",
      "Generating dataset from: images/test/images_frame9800.json\n",
      "Generating dataset from: images/test/images_frame4800.json\n",
      "Generating dataset from: images/test/images_frame8600.json\n",
      "Generating dataset from: images/test/images_frame4450.json\n",
      "Generating dataset from: images/test/images_frame8000.json\n",
      "Generating dataset from: images/test/images_frame4600.json\n",
      "Generating dataset from: images/test/images_frame3600.json\n",
      "Creating dataset: images/val/coco\n",
      "Generating dataset from: images/val/images_frame5300.json\n",
      "Generating dataset from: images/val/images_frame8700.json\n",
      "Generating dataset from: images/val/images_frame9400.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset from: images/val/images_frame1200.json\n",
      "Generating dataset from: images/val/images_frame1050.json\n",
      "Generating dataset from: images/val/images_frame9250.json\n",
      "Generating dataset from: images/val/images_frame1250.json\n",
      "Generating dataset from: images/val/images_frame4000.json\n",
      "Generating dataset from: images/val/images_frame3200.json\n",
      "Generating dataset from: images/val/images_frame9350.json\n",
      "Generating dataset from: images/val/images_frame2200.json\n",
      "Generating dataset from: images/val/images_frame7050.json\n",
      "Generating dataset from: images/val/images_frame4250.json\n",
      "Generating dataset from: images/val/images_frame2400.json\n",
      "Generating dataset from: images/val/images_frame7850.json\n",
      "Generating dataset from: images/val/images_frame8550.json\n",
      "Generating dataset from: images/val/images_frame4300.json\n",
      "Generating dataset from: images/val/images_frame5750.json\n",
      "Generating dataset from: images/val/images_frame4050.json\n",
      "Generating dataset from: images/val/images_frame3050.json\n",
      "Generating dataset from: images/val/images_frame1600.json\n",
      "Generating dataset from: images/val/images_frame9850.json\n",
      "Generating dataset from: images/val/images_frame10200.json\n",
      "Generating dataset from: images/val/images_frame4700.json\n"
     ]
    }
   ],
   "source": [
    "# vamos a convertir estas fotos en y anotaciones en datasets de formato COCO. Tenemos que hacerlo para los sets de train\n",
    "# test y validación. Tenemos que pasar como argumento archivo txt con los labels siguiendo el formato indiado en la\n",
    "# documentación oficial de labelme https://github.com/wkentaro/labelme/tree/master/examples/instance_segmentation. Un ejemplo\n",
    "# de cómo debe ser este archivo se muestra debajo\n",
    "!python3 scripts/labelme2coco.py images/train images/train/coco --labels training/labels.txt\n",
    "!python3 scripts/labelme2coco.py images/test images/test/coco --labels training/labels.txt\n",
    "!python3 scripts/labelme2coco.py images/val images/val/coco --labels training/labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"training/ejemplo_labels_txt.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora borramos las imágenes y anotaciones de las carpetas train, test y val, ya que la carpeta coco dentro de cada \n",
    "# una de estas carpetas ya contiene las imágenes origina\n",
    "!rm images/test/*\n",
    "!mv images/test/coco/* images/test\n",
    "!rm -rf images/test/coco \n",
    "\n",
    "!rm images/train/*\n",
    "!mv images/train/coco/* images/train\n",
    "!rm -rf images/train/coco \n",
    "\n",
    "!rm images/val/*\n",
    "!mv images/val/coco/* images/val\n",
    "!rm -rf images/val/coco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'object_detection/dataset_tools/create_coco_tf_record.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# ahora tenemos que generar los tfrecords usando estos dataset en formato coco\n",
    "!python3 object_detection/dataset_tools/create_coco_tf_record.py --logtostderr \\\n",
    "--train_image_dir=images/train \\\n",
    "--val_image_dir=images/val \\\n",
    "--test_image_dir=images/test \\\n",
    "--train_annotations_file=images/train/annotations.json \\\n",
    "--val_annotations_file=images/val/annotations.json \\\n",
    "--testdev_annotations_file=images/test/annotations.json \\\n",
    "--output_dir=training/tfrecords \\\n",
    "--include_masks=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
